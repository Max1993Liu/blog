{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "# training data is downloaded from\n",
    "# https://github.com/SophonPlus/ChineseNlpCorpus/raw/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip\n",
    "\n",
    "data = pd.read_csv('../data/online_shopping_10_cats.csv')\n",
    "with open('../data/data_for_tokenizer.txt', 'w') as f:\n",
    "    f.writelines([str(i)+'\\n' for i in data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.Train('--input=../data/data_for_tokenizer.txt --model_prefix=m --vocab_size=5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理的火花。整本书的结构颇有特点，从当时（本书写于八十年代）流行的计算机话题引入，再用数学、物理学、宇宙学做必要的铺垫——这些内容占据了大部分篇幅，最后回到关键问题：电脑能不能代替人脑。和现在流行的观点相反，作者认为人的某种“洞察”是不能被算法模拟的。也许作者想说，人的灵魂是无可取代的。\n",
      "['▁', '作者', '真', '有', '英', '国', '人', '严', '谨', '的', '风格', ',', '提出', '观', '点', '、', '进行', '论', '述', '论', '证', ',', '尽', '管', '本人', '对', '物', '理', '学', '了解', '不', '深', ',', '但是', '仍', '然', '能', '感受', '到', '真', '理', '的', '火', '花', '。', '整', '本书', '的', '结', '构', '颇', '有', '特', '点', ',', '从', '当时', '(', '本书', '写', '于', '八', '十', '年', '代', ')', '流', '行', '的', '计', '算', '机', '话', '题', '引', '入', ',', '再', '用', '数', '学', '、', '物', '理', '学', '、', '宇', '宙', '学', '做', '必', '要', '的', '铺', '垫', '——', '这些', '内容', '占', '据', '了', '大', '部分', '篇', '幅', ',', '最后', '回', '到', '关', '键', '问题', ':', '电脑', '能', '不能', '代', '替', '人', '脑', '。', '和', '现在', '流', '行', '的', '观', '点', '相', '反', ',', '作者', '认为', '人的', '某', '种', '“', '洞', '察', '”', '是', '不能', '被', '算', '法', '模', '拟', '的', '。', '也许', '作者', '想', '说', ',', '人的', '灵', '魂', '是', '无', '可', '取', '代', '的', '。']\n"
     ]
    }
   ],
   "source": [
    "# take a look at the tokenizer output\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('./m.model')\n",
    "\n",
    "sample_review = data['review'][1]\n",
    "print(sample_review)\n",
    "print(sp.EncodeAsPieces(sample_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str):\n",
    "    return sp.EncodeAsIds(text)\n",
    "\n",
    "# encode(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![structure](https://img-blog.csdnimg.cn/20190326141457137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(keras.models.Model):\n",
    "    \n",
    "    def __init__(self, region_sizes=None, filter_size=2, emb_size=20, vocab_size=5000, **kwargs):\n",
    "        self.region_sizes = region_sizes or [2, 3, 4]\n",
    "        self.filter_size = filter_size\n",
    "        self.emb_size = emb_size\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__(self, **kwargs)\n",
    "#         self.build()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        sent_length = input_shape[1]\n",
    "        self.emb = keras.layers.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.conv_layers = [keras.layers.Conv1D(filters=self.filter_size, \n",
    "                                                kernel_size=k,\n",
    "                                                activation='relu',\n",
    "                                                padding='same') for k in self.region_sizes]\n",
    "        self.maxpool_layers = [keras.layers.MaxPool1D(sent_length) for _ in range(len(self.region_sizes))]    \n",
    "        self.fc_layers_1 = keras.layers.Dense(128, activation='relu')\n",
    "        self.fc_output = keras.layers.Dense(2, activation='softmax')\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input, training=None):\n",
    "        embeded = self.emb(input)  # [batch_size, sent_length, embedding_size]\n",
    "        conv_output = [c(embeded) for c in self.conv_layers]  # [batch_size, sent_length, embedding_size]\n",
    "        maxpool_output = [p(o) for p, o in zip(self.maxpool_layers, conv_output)] # [batch_size, 1, embdding_size]\n",
    "        \n",
    "        # concat and reshape\n",
    "        output = keras.layers.Concatenate(axis=-1)(maxpool_output)\n",
    "        output = tf.squeeze(output) # [batch_size, emb_size * len(region_size)]\n",
    "        \n",
    "        output = self.fc_layers_1(output)\n",
    "        output = keras.layers.Dropout(rate=0.3)(output, training=training)\n",
    "        output = self.fc_output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(20, 2), dtype=float32, numpy=\n",
       "array([[0.500201  , 0.49979898],\n",
       "       [0.499978  , 0.500022  ],\n",
       "       [0.500201  , 0.49979898],\n",
       "       [0.5020413 , 0.4979587 ],\n",
       "       [0.500201  , 0.49979898],\n",
       "       [0.500201  , 0.49979898],\n",
       "       [0.5010025 , 0.4989975 ],\n",
       "       [0.5016661 , 0.49833384],\n",
       "       [0.50132185, 0.49867812],\n",
       "       [0.5015479 , 0.49845216],\n",
       "       [0.50132185, 0.49867812],\n",
       "       [0.50132185, 0.49867812],\n",
       "       [0.500201  , 0.49979898],\n",
       "       [0.50132185, 0.49867812],\n",
       "       [0.5001629 , 0.4998371 ],\n",
       "       [0.49998584, 0.5000141 ],\n",
       "       [0.5010025 , 0.4989975 ],\n",
       "       [0.50132185, 0.49867812],\n",
       "       [0.500201  , 0.49979898],\n",
       "       [0.50132185, 0.49867812]], dtype=float32)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cnn = TextCNN()\n",
    "\n",
    "\n",
    "sample_data = np.random.randn(20, 50).clip(max=10, min=0)\n",
    "text_cnn(sample_data.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
