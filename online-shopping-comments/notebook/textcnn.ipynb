{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "# training data is downloaded from\n",
    "# https://github.com/SophonPlus/ChineseNlpCorpus/raw/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip\n",
    "\n",
    "data = pd.read_csv('../data/online_shopping_10_cats.csv')\n",
    "# with open('../data/data_for_tokenizer.txt', 'w') as f:\n",
    "#     f.writelines([str(i)+'\\n' for i in data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62774, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "train_command = \"\"\"\n",
    "--input=../data/data_for_tokenizer.txt --model_prefix=m --vocab_size=100000\n",
    "\"\"\"\n",
    "\n",
    "spm.SentencePieceTrainer.Train(train_command.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原文:\n",
      "作者有一种专业的谨慎，若能有幸学习原版也许会更好，简体版的书中的印刷错误比较多，影响学者理解，全书结构简单，但内容详实，学起来如鱼得水非常轻松。这只是一项技术而已，若可以结合本专业，将会得到更高的学习快乐，家财万贯不如一技在身，一技在身不如一念在心，本书有不仅有技，而且有念。书中佳品。\n",
      "\n",
      "分词后:\n",
      "['▁作者', '有一种', '专业的', '谨慎', ',', '若', '能有幸', '学习', '原版', '也许', '会更好', ',', '简体', '版的', '书中的', '印刷错误', '比较多', ',', '影响', '学者', '理解', ',', '全书', '结构', '简单', ',', '但内容', '详实', ',', '学起来', '如鱼得水', '非常', '轻松', '。', '这', '只是', '一项', '技术', '而已', ',', '若', '可以', '结合', '本', '专业', ',', '将会', '得到', '更高的', '学习', '快乐', ',', '家', '财', '万贯', '不如', '一技在身', ',', '一技在身', '不如', '一念', '在', '心', ',', '本书', '有', '不仅有', '技', ',', '而且有', '念', '。', '书中', '佳品', '。']\n"
     ]
    }
   ],
   "source": [
    "# take a look at the tokenizer output\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('./m.model')\n",
    "\n",
    "sample_review = data['review'][5]\n",
    "print('原文:')\n",
    "print(sample_review)\n",
    "print()\n",
    "print('分词后:')\n",
    "print(sp.EncodeAsPieces(sample_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62774, 200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text: str):\n",
    "    encoded = sp.EncodeAsIds(text)\n",
    "    return encoded\n",
    "\n",
    "encoded = [encode(i) for i in data['review'].astype(str)]\n",
    "encoded = keras.preprocessing.sequence.pad_sequences(encoded, maxlen=200, padding='pre')\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![structure](https://img-blog.csdnimg.cn/20190326141457137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper has shown a multi-layer TextCNN architecture, with multiple Conv-BN-Relu Block.\n",
    "\n",
    "https://arxiv.org/pdf/1801.06287.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(keras.models.Model):\n",
    "    \n",
    "    def __init__(self, region_sizes=None, filter_size=2, emb_size=20, vocab_size=5000, **kwargs):\n",
    "        self.region_sizes = region_sizes or [2, 3, 4]\n",
    "        self.filter_size = filter_size\n",
    "        self.emb_size = emb_size\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__(self, **kwargs)\n",
    "#         self.build()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        sent_length = input_shape[1]\n",
    "        self.emb = keras.layers.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.conv_layers = [keras.layers.Conv1D(filters=self.filter_size, \n",
    "                                                kernel_size=k,\n",
    "                                                activation='relu',\n",
    "                                                padding='same') for k in self.region_sizes]\n",
    "        self.maxpool_layers = [keras.layers.MaxPool1D(sent_length) for _ in range(len(self.region_sizes))]    \n",
    "        self.fc_layers_1 = keras.layers.Dense(128, activation='relu')\n",
    "        self.fc_output = keras.layers.Dense(2, activation='softmax')\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input, training=None):\n",
    "        embeded = self.emb(input)  # [batch_size, sent_length, embedding_size]\n",
    "        conv_output = [c(embeded) for c in self.conv_layers]  # [batch_size, sent_length, embedding_size]\n",
    "        maxpool_output = [p(o) for p, o in zip(self.maxpool_layers, conv_output)] # [batch_size, 1, embdding_size]\n",
    "        \n",
    "        # concat and reshape\n",
    "        output = keras.layers.Concatenate(axis=-1)(maxpool_output)\n",
    "        output = keras.layers.Flatten()(output) # [batch_size, emb_size * len(region_size)]\n",
    "        \n",
    "        output = self.fc_layers_1(output)\n",
    "        output = keras.layers.Dropout(rate=0.3)(output, training=training)\n",
    "        output = self.fc_output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cnn = TextCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43941, 200), (18833, 200))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded, data['label'], test_size=0.3, \n",
    "                                                    stratify=data['label'], random_state=1024)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, keras.utils.to_categorical(y_train)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, keras.utils.to_categorical(y_test)))\n",
    "\n",
    "train_dataset = train_dataset.batch(32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n",
       "array([[0.49919888, 0.50080115],\n",
       "       [0.4969505 , 0.50304943],\n",
       "       [0.49848965, 0.5015103 ],\n",
       "       [0.49731615, 0.50268376],\n",
       "       [0.49868202, 0.501318  ],\n",
       "       [0.500227  , 0.499773  ],\n",
       "       [0.4983687 , 0.5016313 ],\n",
       "       [0.4988659 , 0.50113416],\n",
       "       [0.49807504, 0.50192493],\n",
       "       [0.49875778, 0.5012422 ],\n",
       "       [0.50001603, 0.49998394],\n",
       "       [0.4986122 , 0.5013877 ],\n",
       "       [0.49960265, 0.5003973 ],\n",
       "       [0.4994362 , 0.50056386],\n",
       "       [0.4998263 , 0.5001737 ],\n",
       "       [0.49859416, 0.50140584],\n",
       "       [0.49901065, 0.5009893 ],\n",
       "       [0.49835193, 0.50164807],\n",
       "       [0.49991155, 0.5000884 ],\n",
       "       [0.4993303 , 0.50066966],\n",
       "       [0.49856788, 0.5014321 ],\n",
       "       [0.49830568, 0.50169426],\n",
       "       [0.49938536, 0.50061464],\n",
       "       [0.49940613, 0.5005939 ],\n",
       "       [0.501117  , 0.498883  ],\n",
       "       [0.5000554 , 0.49994466],\n",
       "       [0.50042486, 0.49957517],\n",
       "       [0.49873164, 0.5012683 ],\n",
       "       [0.4976803 , 0.50231975],\n",
       "       [0.49951866, 0.5004813 ],\n",
       "       [0.49954182, 0.5004582 ],\n",
       "       [0.49813285, 0.5018672 ]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = next(iter(train_dataset))\n",
    "text_cnn(sample_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43941 samples\n",
      "Epoch 1/10\n",
      "43941/43941 [==============================] - 9s 200us/sample - loss: 0.3185\n",
      "Epoch 2/10\n",
      "43941/43941 [==============================] - 8s 185us/sample - loss: 0.1872\n",
      "Epoch 3/10\n",
      "43941/43941 [==============================] - 8s 186us/sample - loss: 0.1444\n",
      "Epoch 4/10\n",
      "43941/43941 [==============================] - 8s 176us/sample - loss: 0.1117\n",
      "Epoch 5/10\n",
      "43941/43941 [==============================] - 8s 180us/sample - loss: 0.0869\n",
      "Epoch 6/10\n",
      "43941/43941 [==============================] - 8s 190us/sample - loss: 0.0716\n",
      "Epoch 7/10\n",
      "43941/43941 [==============================] - 9s 204us/sample - loss: 0.0554\n",
      "Epoch 8/10\n",
      "43941/43941 [==============================] - 8s 180us/sample - loss: 0.0448\n",
      "Epoch 9/10\n",
      "43941/43941 [==============================] - 8s 177us/sample - loss: 0.0397\n",
      "Epoch 10/10\n",
      "43941/43941 [==============================] - 8s 176us/sample - loss: 0.0334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f7067f0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cnn.compile(optimizer=keras.optimizers.Adam(), \n",
    "                 loss='categorical_crossentropy')\n",
    "\n",
    "text_cnn.fit(X_train, keras.utils.to_categorical(y_train), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
