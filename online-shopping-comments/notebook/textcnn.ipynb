{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "# training data is downloaded from\n",
    "# https://github.com/SophonPlus/ChineseNlpCorpus/raw/master/datasets/online_shopping_10_cats/online_shopping_10_cats.zip\n",
    "\n",
    "data = pd.read_csv('../data/online_shopping_10_cats.csv')\n",
    "# with open('../data/data_for_tokenizer.txt', 'w') as f:\n",
    "#     f.writelines([str(i)+'\\n' for i in data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train our tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sentencepiece as spm\n",
    "# spm.SentencePieceTrainer.Train('--input=../data/data_for_tokenizer.txt --model_prefix=m --vocab_size=5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到真理的火花。整本书的结构颇有特点，从当时（本书写于八十年代）流行的计算机话题引入，再用数学、物理学、宇宙学做必要的铺垫——这些内容占据了大部分篇幅，最后回到关键问题：电脑能不能代替人脑。和现在流行的观点相反，作者认为人的某种“洞察”是不能被算法模拟的。也许作者想说，人的灵魂是无可取代的。\n",
      "['▁', '作者', '真', '有', '英', '国', '人', '严', '谨', '的', '风格', ',', '提出', '观', '点', '、', '进行', '论', '述', '论', '证', ',', '尽', '管', '本人', '对', '物', '理', '学', '了解', '不', '深', ',', '但是', '仍', '然', '能', '感受', '到', '真', '理', '的', '火', '花', '。', '整', '本书', '的', '结', '构', '颇', '有', '特', '点', ',', '从', '当时', '(', '本书', '写', '于', '八', '十', '年', '代', ')', '流', '行', '的', '计', '算', '机', '话', '题', '引', '入', ',', '再', '用', '数', '学', '、', '物', '理', '学', '、', '宇', '宙', '学', '做', '必', '要', '的', '铺', '垫', '——', '这些', '内容', '占', '据', '了', '大', '部分', '篇', '幅', ',', '最后', '回', '到', '关', '键', '问题', ':', '电脑', '能', '不能', '代', '替', '人', '脑', '。', '和', '现在', '流', '行', '的', '观', '点', '相', '反', ',', '作者', '认为', '人的', '某', '种', '“', '洞', '察', '”', '是', '不能', '被', '算', '法', '模', '拟', '的', '。', '也许', '作者', '想', '说', ',', '人的', '灵', '魂', '是', '无', '可', '取', '代', '的', '。']\n"
     ]
    }
   ],
   "source": [
    "# take a look at the tokenizer output\n",
    "import sentencepiece as spm\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('./m.model')\n",
    "\n",
    "sample_review = data['review'][1]\n",
    "print(sample_review)\n",
    "print(sp.EncodeAsPieces(sample_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62774, 200)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text: str):\n",
    "    encoded = sp.EncodeAsIds(text)\n",
    "    return encoded\n",
    "\n",
    "\n",
    "encoded = [encode(i) for i in data['review'].astype(str)]\n",
    "encoded = keras.preprocessing.sequence.pad_sequences(encoded, maxlen=200, padding='pre')\n",
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![structure](https://img-blog.csdnimg.cn/20190326141457137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2FzaWFsZWVfYmlyZA==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper has shown a multi-layer TextCNN architecture, with multiple Conv-BN-Relu Block.\n",
    "\n",
    "https://arxiv.org/pdf/1801.06287.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(keras.models.Model):\n",
    "    \n",
    "    def __init__(self, region_sizes=None, filter_size=2, emb_size=20, vocab_size=5000, **kwargs):\n",
    "        self.region_sizes = region_sizes or [2, 3, 4]\n",
    "        self.filter_size = filter_size\n",
    "        self.emb_size = emb_size\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__(self, **kwargs)\n",
    "#         self.build()\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        sent_length = input_shape[1]\n",
    "        self.emb = keras.layers.Embedding(self.vocab_size, self.emb_size)\n",
    "        self.conv_layers = [keras.layers.Conv1D(filters=self.filter_size, \n",
    "                                                kernel_size=k,\n",
    "                                                activation='relu',\n",
    "                                                padding='same') for k in self.region_sizes]\n",
    "        self.maxpool_layers = [keras.layers.MaxPool1D(sent_length) for _ in range(len(self.region_sizes))]    \n",
    "        self.fc_layers_1 = keras.layers.Dense(128, activation='relu')\n",
    "        self.fc_output = keras.layers.Dense(2, activation='softmax')\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input, training=None):\n",
    "        embeded = self.emb(input)  # [batch_size, sent_length, embedding_size]\n",
    "        conv_output = [c(embeded) for c in self.conv_layers]  # [batch_size, sent_length, embedding_size]\n",
    "        maxpool_output = [p(o) for p, o in zip(self.maxpool_layers, conv_output)] # [batch_size, 1, embdding_size]\n",
    "        \n",
    "        # concat and reshape\n",
    "        output = keras.layers.Concatenate(axis=-1)(maxpool_output)\n",
    "        output = keras.layers.Flatten()(output) # [batch_size, emb_size * len(region_size)]\n",
    "        \n",
    "        output = self.fc_layers_1(output)\n",
    "        output = keras.layers.Dropout(rate=0.3)(output, training=training)\n",
    "        output = self.fc_output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cnn = TextCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((43941, 200), (18833, 200))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded, data['label'], test_size=0.3, \n",
    "                                                    stratify=data['label'], random_state=1024)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, keras.utils.to_categorical(y_train)))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, keras.utils.to_categorical(y_test)))\n",
    "\n",
    "train_dataset = train_dataset.batch(32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 2), dtype=float32, numpy=\n",
       "array([[0.49919888, 0.50080115],\n",
       "       [0.4969505 , 0.50304943],\n",
       "       [0.49848965, 0.5015103 ],\n",
       "       [0.49731615, 0.50268376],\n",
       "       [0.49868202, 0.501318  ],\n",
       "       [0.500227  , 0.499773  ],\n",
       "       [0.4983687 , 0.5016313 ],\n",
       "       [0.4988659 , 0.50113416],\n",
       "       [0.49807504, 0.50192493],\n",
       "       [0.49875778, 0.5012422 ],\n",
       "       [0.50001603, 0.49998394],\n",
       "       [0.4986122 , 0.5013877 ],\n",
       "       [0.49960265, 0.5003973 ],\n",
       "       [0.4994362 , 0.50056386],\n",
       "       [0.4998263 , 0.5001737 ],\n",
       "       [0.49859416, 0.50140584],\n",
       "       [0.49901065, 0.5009893 ],\n",
       "       [0.49835193, 0.50164807],\n",
       "       [0.49991155, 0.5000884 ],\n",
       "       [0.4993303 , 0.50066966],\n",
       "       [0.49856788, 0.5014321 ],\n",
       "       [0.49830568, 0.50169426],\n",
       "       [0.49938536, 0.50061464],\n",
       "       [0.49940613, 0.5005939 ],\n",
       "       [0.501117  , 0.498883  ],\n",
       "       [0.5000554 , 0.49994466],\n",
       "       [0.50042486, 0.49957517],\n",
       "       [0.49873164, 0.5012683 ],\n",
       "       [0.4976803 , 0.50231975],\n",
       "       [0.49951866, 0.5004813 ],\n",
       "       [0.49954182, 0.5004582 ],\n",
       "       [0.49813285, 0.5018672 ]], dtype=float32)>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = next(iter(train_dataset))\n",
    "text_cnn(sample_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 43941 samples\n",
      "Epoch 1/10\n",
      "43941/43941 [==============================] - 9s 200us/sample - loss: 0.3185\n",
      "Epoch 2/10\n",
      "43941/43941 [==============================] - 8s 185us/sample - loss: 0.1872\n",
      "Epoch 3/10\n",
      "43941/43941 [==============================] - 8s 186us/sample - loss: 0.1444\n",
      "Epoch 4/10\n",
      "43941/43941 [==============================] - 8s 176us/sample - loss: 0.1117\n",
      "Epoch 5/10\n",
      "43941/43941 [==============================] - 8s 180us/sample - loss: 0.0869\n",
      "Epoch 6/10\n",
      "43941/43941 [==============================] - 8s 190us/sample - loss: 0.0716\n",
      "Epoch 7/10\n",
      "43941/43941 [==============================] - 9s 204us/sample - loss: 0.0554\n",
      "Epoch 8/10\n",
      "43941/43941 [==============================] - 8s 180us/sample - loss: 0.0448\n",
      "Epoch 9/10\n",
      "43941/43941 [==============================] - 8s 177us/sample - loss: 0.0397\n",
      "Epoch 10/10\n",
      "43941/43941 [==============================] - 8s 176us/sample - loss: 0.0334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14f7067f0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cnn.compile(optimizer=keras.optimizers.Adam(), \n",
    "                 loss='categorical_crossentropy')\n",
    "\n",
    "text_cnn.fit(X_train, keras.utils.to_categorical(y_train), epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
