# Part 1: GPT2及其他的语言模型


### 什么是语言模型（Language model)?
简单来说，语言模型就是**通过输入句子的一部分来预测下一个词的模型**。一个常见的例子就是手机输入法，输入法根据你当前的输入来推荐接下来可能的词语。

所以从某种角度上来看，GPT-2和之前的其他模型（例如n-gram）的目标是相同的，只是它采用了一个更加复杂的模型。GPT-2通过一个叫**WebText**的巨大数据集进行训练，最小的GPT-2模型也有500MB,而最大的GPT模型更是达到了6.5GB。

[](!gpt2-sizes.png)

### 基于transformer结构的语言模型
从下图中可以看到，transformer结构在创建伊始主要用来解决的事机器翻译的问题，因此其只要结构包含编码器（encoder）和解码器（decoder）。

【encoder-decoder]照片

随着对transformer结构研究的深入，研究者们发现单独使用encoder或者decoder的结构，并且将这样的结构重复的越叠越高（堆叠的高度也决定了模型的大小和参数量），用海量的数据进行训练，然后用大量的计算资源进行训练（大把烧钱），就能够得到好模型。

[gpt2 bert xl图片]

那这些同样的结构到底叠了多高呢：

【堆叠的图片、


