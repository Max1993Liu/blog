{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow>=2.2.0\n",
    "# !pip install tensorflow_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of VAE on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 60000\n",
    "batch_size = 32\n",
    "test_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use mnist as example\n",
    "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "def preprocess_images(images):\n",
    "    images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
    "    return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
    "\n",
    "train_images = preprocess_images(train_images)\n",
    "test_images = preprocess_images(test_images)\n",
    "\n",
    "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
    "                 .shuffle(train_size).batch(batch_size))\n",
    "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
    "                .shuffle(test_size).batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x63f795610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0].squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "      axis=raxis)\n",
    "\n",
    "\n",
    "class VAE(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, latent_dim, opt=None):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.opt = opt or tf.keras.optimizers.Adam(1e-4)\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                # No activation\n",
    "                tf.keras.layers.Dense(latent_dim + latent_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
    "                tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
    "                tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32, kernel_size=3, strides=2, padding='same',\n",
    "                    activation='relu'),\n",
    "                # No activation\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=3, strides=1, padding='same'),\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def decode(self, z):\n",
    "        logits = self.decoder(z)\n",
    "        return logits\n",
    "#         return tf.sigmoid(logits)\n",
    "    \n",
    "    @tf.function\n",
    "    def sample(self, n=100, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(n, self.latent_dim))\n",
    "        return self.decode(eps)\n",
    "    \n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * 0.5) + mean\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_loss(self, x):\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = model.reparameterize(mean, logvar)\n",
    "        x_logit = self.decode(z)\n",
    "        cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "        logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
    "        logpz = log_normal_pdf(z, 0., 0.)\n",
    "        logqz_x = log_normal_pdf(z, mean, logvar)\n",
    "        return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "    \n",
    "    @tf.function\n",
    "    def train(self, x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.compute_loss(x)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(latent_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss: 542.686767578125\n",
      "Epoch 0, Step 100, Loss: 407.926513671875\n",
      "Epoch 0, Step 200, Loss: 264.59503173828125\n",
      "Epoch 0, Step 300, Loss: 219.7979736328125\n",
      "Epoch 0, Step 400, Loss: 210.24435424804688\n",
      "Epoch 0, Step 500, Loss: 201.87814331054688\n",
      "Epoch 0, Step 600, Loss: 207.2767333984375\n",
      "Epoch 0, Step 700, Loss: 202.0150604248047\n",
      "Epoch 0, Step 800, Loss: 203.4485321044922\n",
      "Epoch 0, Step 900, Loss: 196.92193603515625\n",
      "Epoch 0, Step 1000, Loss: 194.33486938476562\n",
      "Epoch 0, Step 1100, Loss: 187.56553649902344\n",
      "Epoch 0, Step 1200, Loss: 176.79830932617188\n",
      "Epoch 0, Step 1300, Loss: 171.27064514160156\n",
      "Epoch 0, Step 1400, Loss: 183.76953125\n",
      "Epoch 0, Step 1500, Loss: 175.10414123535156\n",
      "Epoch 0, Step 1600, Loss: 176.72967529296875\n",
      "Epoch 0, Step 1700, Loss: 167.51351928710938\n",
      "Epoch 0, Step 1800, Loss: 173.41702270507812\n",
      "Epoch 1, Step 0, Loss: 186.98016357421875\n",
      "Epoch 1, Step 100, Loss: 164.63099670410156\n",
      "Epoch 1, Step 200, Loss: 159.73471069335938\n",
      "Epoch 1, Step 300, Loss: 153.12258911132812\n",
      "Epoch 1, Step 400, Loss: 149.81629943847656\n",
      "Epoch 1, Step 500, Loss: 156.98963928222656\n",
      "Epoch 1, Step 600, Loss: 157.05410766601562\n",
      "Epoch 1, Step 700, Loss: 159.7388916015625\n",
      "Epoch 1, Step 800, Loss: 169.67184448242188\n",
      "Epoch 1, Step 900, Loss: 157.05313110351562\n",
      "Epoch 1, Step 1000, Loss: 166.78575134277344\n",
      "Epoch 1, Step 1100, Loss: 166.4695587158203\n",
      "Epoch 1, Step 1200, Loss: 147.14413452148438\n",
      "Epoch 1, Step 1300, Loss: 160.50775146484375\n",
      "Epoch 1, Step 1400, Loss: 149.069091796875\n",
      "Epoch 1, Step 1500, Loss: 164.1848602294922\n",
      "Epoch 1, Step 1600, Loss: 145.836181640625\n",
      "Epoch 1, Step 1700, Loss: 150.9072723388672\n",
      "Epoch 1, Step 1800, Loss: 161.129638671875\n",
      "Epoch 2, Step 0, Loss: 163.34730529785156\n",
      "Epoch 2, Step 100, Loss: 148.7711639404297\n",
      "Epoch 2, Step 200, Loss: 147.87353515625\n",
      "Epoch 2, Step 300, Loss: 153.748779296875\n",
      "Epoch 2, Step 400, Loss: 153.5479736328125\n",
      "Epoch 2, Step 500, Loss: 139.76275634765625\n",
      "Epoch 2, Step 600, Loss: 154.49871826171875\n",
      "Epoch 2, Step 700, Loss: 153.16741943359375\n",
      "Epoch 2, Step 800, Loss: 143.1575164794922\n",
      "Epoch 2, Step 900, Loss: 136.2965545654297\n",
      "Epoch 2, Step 1000, Loss: 149.1377410888672\n",
      "Epoch 2, Step 1100, Loss: 148.495361328125\n",
      "Epoch 2, Step 1200, Loss: 151.64491271972656\n",
      "Epoch 2, Step 1300, Loss: 156.39508056640625\n",
      "Epoch 2, Step 1400, Loss: 161.1668243408203\n",
      "Epoch 2, Step 1500, Loss: 133.95596313476562\n",
      "Epoch 2, Step 1600, Loss: 142.68203735351562\n",
      "Epoch 2, Step 1700, Loss: 148.753173828125\n",
      "Epoch 2, Step 1800, Loss: 142.13075256347656\n",
      "Epoch 3, Step 0, Loss: 157.60745239257812\n",
      "Epoch 3, Step 100, Loss: 153.46029663085938\n",
      "Epoch 3, Step 200, Loss: 151.8968505859375\n",
      "Epoch 3, Step 300, Loss: 151.77157592773438\n",
      "Epoch 3, Step 400, Loss: 142.6728057861328\n",
      "Epoch 3, Step 500, Loss: 140.26629638671875\n",
      "Epoch 3, Step 600, Loss: 151.50497436523438\n",
      "Epoch 3, Step 700, Loss: 142.20733642578125\n",
      "Epoch 3, Step 800, Loss: 133.9871368408203\n",
      "Epoch 3, Step 900, Loss: 132.8052520751953\n",
      "Epoch 3, Step 1000, Loss: 142.62933349609375\n",
      "Epoch 3, Step 1100, Loss: 158.82223510742188\n",
      "Epoch 3, Step 1200, Loss: 151.55213928222656\n",
      "Epoch 3, Step 1300, Loss: 145.35787963867188\n",
      "Epoch 3, Step 1400, Loss: 151.8812255859375\n",
      "Epoch 3, Step 1500, Loss: 147.18826293945312\n",
      "Epoch 3, Step 1600, Loss: 137.38864135742188\n",
      "Epoch 3, Step 1700, Loss: 141.8302459716797\n",
      "Epoch 3, Step 1800, Loss: 136.40646362304688\n",
      "Epoch 4, Step 0, Loss: 144.13986206054688\n",
      "Epoch 4, Step 100, Loss: 150.49342346191406\n",
      "Epoch 4, Step 200, Loss: 146.91590881347656\n",
      "Epoch 4, Step 300, Loss: 139.55599975585938\n",
      "Epoch 4, Step 400, Loss: 132.05047607421875\n",
      "Epoch 4, Step 500, Loss: 137.1151123046875\n",
      "Epoch 4, Step 600, Loss: 142.81138610839844\n",
      "Epoch 4, Step 700, Loss: 156.41847229003906\n",
      "Epoch 4, Step 800, Loss: 138.6715087890625\n",
      "Epoch 4, Step 900, Loss: 153.18797302246094\n",
      "Epoch 4, Step 1000, Loss: 140.5245819091797\n",
      "Epoch 4, Step 1100, Loss: 163.125244140625\n",
      "Epoch 4, Step 1200, Loss: 148.98403930664062\n",
      "Epoch 4, Step 1300, Loss: 156.7497100830078\n",
      "Epoch 4, Step 1400, Loss: 122.6655044555664\n",
      "Epoch 4, Step 1500, Loss: 142.96774291992188\n",
      "Epoch 4, Step 1600, Loss: 145.62200927734375\n",
      "Epoch 4, Step 1700, Loss: 152.47108459472656\n",
      "Epoch 4, Step 1800, Loss: 139.8643798828125\n",
      "Epoch 5, Step 0, Loss: 155.3446502685547\n",
      "Epoch 5, Step 100, Loss: 138.34356689453125\n",
      "Epoch 5, Step 200, Loss: 127.98287200927734\n",
      "Epoch 5, Step 300, Loss: 153.77928161621094\n",
      "Epoch 5, Step 400, Loss: 146.04714965820312\n",
      "Epoch 5, Step 500, Loss: 135.91268920898438\n",
      "Epoch 5, Step 600, Loss: 131.79824829101562\n",
      "Epoch 5, Step 700, Loss: 140.21444702148438\n",
      "Epoch 5, Step 800, Loss: 145.09255981445312\n",
      "Epoch 5, Step 900, Loss: 151.74058532714844\n",
      "Epoch 5, Step 1000, Loss: 147.505126953125\n",
      "Epoch 5, Step 1100, Loss: 144.5439453125\n",
      "Epoch 5, Step 1200, Loss: 145.96376037597656\n",
      "Epoch 5, Step 1300, Loss: 128.13937377929688\n",
      "Epoch 5, Step 1400, Loss: 156.79835510253906\n",
      "Epoch 5, Step 1500, Loss: 146.30877685546875\n",
      "Epoch 5, Step 1600, Loss: 148.2559051513672\n",
      "Epoch 5, Step 1700, Loss: 126.27369689941406\n",
      "Epoch 5, Step 1800, Loss: 148.13394165039062\n",
      "Epoch 6, Step 0, Loss: 153.88284301757812\n",
      "Epoch 6, Step 100, Loss: 135.06582641601562\n",
      "Epoch 6, Step 200, Loss: 140.8702392578125\n",
      "Epoch 6, Step 300, Loss: 137.63909912109375\n",
      "Epoch 6, Step 400, Loss: 157.48385620117188\n",
      "Epoch 6, Step 500, Loss: 133.4044952392578\n",
      "Epoch 6, Step 600, Loss: 128.55117797851562\n",
      "Epoch 6, Step 700, Loss: 136.2996826171875\n",
      "Epoch 6, Step 800, Loss: 147.1484375\n",
      "Epoch 6, Step 900, Loss: 129.26622009277344\n",
      "Epoch 6, Step 1000, Loss: 131.9381103515625\n",
      "Epoch 6, Step 1100, Loss: 138.87564086914062\n",
      "Epoch 6, Step 1200, Loss: 144.06591796875\n",
      "Epoch 6, Step 1300, Loss: 133.35079956054688\n",
      "Epoch 6, Step 1400, Loss: 134.75172424316406\n",
      "Epoch 6, Step 1500, Loss: 114.00149536132812\n",
      "Epoch 6, Step 1600, Loss: 120.04864501953125\n",
      "Epoch 6, Step 1700, Loss: 118.83168029785156\n",
      "Epoch 6, Step 1800, Loss: 140.65451049804688\n",
      "Epoch 7, Step 0, Loss: 131.25131225585938\n",
      "Epoch 7, Step 100, Loss: 155.0531463623047\n",
      "Epoch 7, Step 200, Loss: 138.56307983398438\n",
      "Epoch 7, Step 300, Loss: 139.4593963623047\n",
      "Epoch 7, Step 400, Loss: 134.9582977294922\n",
      "Epoch 7, Step 500, Loss: 134.85067749023438\n",
      "Epoch 7, Step 600, Loss: 136.57717895507812\n",
      "Epoch 7, Step 700, Loss: 135.8787841796875\n",
      "Epoch 7, Step 800, Loss: 135.91476440429688\n",
      "Epoch 7, Step 900, Loss: 130.19895935058594\n",
      "Epoch 7, Step 1000, Loss: 131.23033142089844\n",
      "Epoch 7, Step 1100, Loss: 152.85586547851562\n",
      "Epoch 7, Step 1200, Loss: 136.10723876953125\n",
      "Epoch 7, Step 1300, Loss: 128.593017578125\n",
      "Epoch 7, Step 1400, Loss: 150.12240600585938\n",
      "Epoch 7, Step 1500, Loss: 141.82237243652344\n",
      "Epoch 7, Step 1600, Loss: 135.49986267089844\n",
      "Epoch 7, Step 1700, Loss: 142.18170166015625\n",
      "Epoch 7, Step 1800, Loss: 144.46556091308594\n",
      "Epoch 8, Step 0, Loss: 141.3543701171875\n",
      "Epoch 8, Step 100, Loss: 136.35450744628906\n",
      "Epoch 8, Step 200, Loss: 151.04782104492188\n",
      "Epoch 8, Step 300, Loss: 137.38543701171875\n",
      "Epoch 8, Step 400, Loss: 134.94537353515625\n",
      "Epoch 8, Step 500, Loss: 136.895263671875\n",
      "Epoch 8, Step 600, Loss: 150.18153381347656\n",
      "Epoch 8, Step 700, Loss: 144.57749938964844\n",
      "Epoch 8, Step 800, Loss: 130.5440673828125\n",
      "Epoch 8, Step 900, Loss: 148.21124267578125\n",
      "Epoch 8, Step 1000, Loss: 150.71905517578125\n",
      "Epoch 8, Step 1100, Loss: 143.30398559570312\n",
      "Epoch 8, Step 1200, Loss: 165.66104125976562\n",
      "Epoch 8, Step 1300, Loss: 150.35357666015625\n",
      "Epoch 8, Step 1400, Loss: 157.04110717773438\n",
      "Epoch 8, Step 1500, Loss: 150.63644409179688\n",
      "Epoch 8, Step 1600, Loss: 158.2554473876953\n",
      "Epoch 8, Step 1700, Loss: 142.87850952148438\n",
      "Epoch 8, Step 1800, Loss: 130.61766052246094\n",
      "Epoch 9, Step 0, Loss: 149.14315795898438\n",
      "Epoch 9, Step 100, Loss: 145.8337860107422\n",
      "Epoch 9, Step 200, Loss: 130.87400817871094\n",
      "Epoch 9, Step 300, Loss: 133.25218200683594\n",
      "Epoch 9, Step 400, Loss: 146.25997924804688\n",
      "Epoch 9, Step 500, Loss: 145.71279907226562\n",
      "Epoch 9, Step 600, Loss: 149.73904418945312\n",
      "Epoch 9, Step 700, Loss: 144.7948455810547\n",
      "Epoch 9, Step 800, Loss: 132.6101837158203\n",
      "Epoch 9, Step 900, Loss: 144.55877685546875\n",
      "Epoch 9, Step 1000, Loss: 139.5421600341797\n",
      "Epoch 9, Step 1100, Loss: 147.50706481933594\n",
      "Epoch 9, Step 1200, Loss: 130.19989013671875\n",
      "Epoch 9, Step 1300, Loss: 129.18759155273438\n",
      "Epoch 9, Step 1400, Loss: 154.0364227294922\n",
      "Epoch 9, Step 1500, Loss: 148.97982788085938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Step 1600, Loss: 114.23011779785156\n",
      "Epoch 9, Step 1700, Loss: 149.72698974609375\n",
      "Epoch 9, Step 1800, Loss: 138.80377197265625\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, x in enumerate(train_dataset):\n",
    "        loss = model.train(x)\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {e}, Step {i}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x63f8c7b10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(model.sample(n=2)[1].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
